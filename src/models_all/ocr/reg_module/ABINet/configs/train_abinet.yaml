global:
  name: train-abinet
  phase: train
  stage: train-super
  workdir: workdir
  seed: ~
 
dataset:
  train: {
    roots: ['data/wordbase/biasach/', 
            'data/wordbase/block_train', 
            'data/wordbase/saoke',
            'data/wordbase/syn_200k',
            'data/wordbase/tdsd_ver4',
            'data/wordbase/tdsd_ver5',
            'data/wordbase/tdsd_ver6',
            'data/wordbase/tdsd_ver7',
            'data/wordbase/textocr',
            'data/wordbase/tphcm_train',
            'data/wordbase/vintext',
            'data/wordbase/cdp',
            'data/wordbase/layoutlm',
            'data/wordbase/cord_train',
            'data/wordbase/cord_test',
            'data/wordbase/cord_val'],
    batch_size: 128
  }
  test: {
    roots: ['data/wordbase/block_val',
            'data/wordbase/s90k_val',
            'data/wordbase/tphcm_val',
            'data/wordbase/tdsd_ver3'],
    batch_size: 128
  }
  data_aug: True
  multiscales: False
  num_workers: 14

training:
  epochs: 10
  show_iters: 50
  eval_iters: 3000
  save_iters: 3000

optimizer:
  type: Adam
  true_wd: False
  wd: 0.0
  bn_wd: False
  clip_grad: 20
  lr: 0.0001
  args: {
    betas: !!python/tuple [0.9, 0.999], # for default Adam 
  }
  scheduler: {
    periods: [6, 4],
    gamma: 0.1,
  }

model:
  name: 'modules.model_abinet_iter.ABINetIterModel'
  iter_size: 3
  ensemble: ''
  use_vision: False
  vision: {
    checkpoint: workdir/pretrain-vision-model/best-pretrain-vision-model.pth,
    loss_weight: 1.,
    attention: 'position',
    backbone: 'transformer',
    backbone_ln: 3,
  }
  language: {
    checkpoint:  workdir/pretrain-language-model/best-pretrain-language-model.pth,
    num_layers: 4,
    loss_weight: 1.,
    detach: True,
    use_self_attn: False
  }
  alignment: {
    loss_weight: 1.,
  }
